# experiment config
experiment: 
  type: "classification"
  log: true             # can change where logs are stored (currently not used)
  name: "rnn-alt-acq"           # experiment name for wandb group

# dataset config
dataset: 
  name: "AGNews"
  seed_size: 50          # number of training examples to start with
  seed_balance: "balanced"
  pool_balance: "balanced"
  # imbalance_prop: 0.2
  # imbalance_cls: 0
  seed: 123
  val_prop: 0.2           # proportion of data to use for validation (randomly sampled)
  debug: false             # reduced dataset size for debugging

# acquisition model config
acq_model:
  model_type: "RNN-hid"
  model_hypers:
    architecture:
      pretrained_emb: "glove"    # pretrained word embeddings: ["glove", "word2vec"]
      max_length: 300           # max sequence length
      fine_tune: false          # fine tune embeddings
      hid_dim: 64         # size of RNN latent state
      head_dim: 64        # size of hidden dimension on output head (not used)
      num_layers: 1       # number of RNN layers
      dropout: 0.2        
    optim:
      epochs: 200           # max number of epochs
      lr: 0.001
      batch_size: 16
      optimizer: "adam"
      scheduler: "constant"
      warmup_steps: 0
      loss: "CE"            # can change to unbiased estimators here?
      L2_reg: 0.01          # weight decay
      patience: 10           # early stopping
      num_workers: 0        # number of dataloader workers
      momentum: 0.9     # not used for adam
      
      
# evaluation model config
eval_models:
  - model_type: "RNN"
    model_hypers:
      architecture:
        pretrained_emb: "glove"    # pretrained word embeddings: ["glove", "word2vec"]
        max_length: 300           # max sequence length
        fine_tune: false          # fine tune embeddings
        hid_dim: 64         # size of RNN latent state
        head_dim: 64        # size of hidden dimension on output head (not used)
        num_layers: 1       # number of RNN layers
        dropout: 0.2        
      optim:
        epochs: 200           # max number of epochs
        lr: 0.001
        batch_size: 16
        optimizer: "adam"
        scheduler: "constant"
        warmup_steps: 0
        loss: "CE"            # can change to unbiased estimators here?
        L2_reg: 0.01          # weight decay
        patience: 10           # early stopping
        num_workers: 0        # number of dataloader workers
        momentum: 0.9     # not used for adam  
  
# query strategy config
query_strategy:
  query_type: "random"          
  query_size: 50             # number of points to label per acquisition step
  num_queries: 20            # if 0 then use full train/val sets
  # num_subsample: 5000        # score only a select subsample
  num_mc_iters: 20          # number of monte-carlo samples for dropout

# ray config
