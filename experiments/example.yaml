# experiment config
experiment: 
  type: "classification"
  log: true              # can change where logs are stored (currently not used)
  num_repetitions: 5       # number of runs to average over
  name: "entropy"           # experiment name for wandb group

# dataset config
dataset: 
  name: "AGNews"
  seed_fraction: 0.02
  seed_balance: "balanced"
  # imbalance_prop: 0.2
  # imbalance_cls: 0
  seed: 123
  val_prop: 0.1
  debug: false             # reduced dataset size for debugging

# acquisition model config
acq_model:
  model_type: "BERT"
  model_hypers:
    # specific to the model? Seperate yaml config for model params?
    architecture:
      pretrained_model: "bert-base-uncased"
      fine_tune: true
      dropout: 0.5
    optim:
      epochs: 25           # max number of epochs
      lr: 1.0e-5
      batch_size: 8
      optimizer: "adamw"
      scheduler: "constant"
      warmup_steps: 10     # scheduler warm up
      loss: "CE"            # can change to unbiased estimators here?
      L2_reg: 0.005          # weight decay
      patience: 2           # early stopping
      num_workers: 0        # number of dataloader workers
      momentum: 0.9     # not used for adam
      
# evaluation model config
eval_models:
  - model_type: "BERT"
    model_hypers:
      # specific to the model? Seperate yaml config for model params?
      architecture:
        pretrained_model: "bert-base-uncased"
        fine_tune: true
        dropout: 0.1
      optim:
        epochs: 25           # max number of epochs
        lr: 5.0e-5
        batch_size: 16
        optimizer: "adamw"
        scheduler: "constant"
        warmup_steps: 10     # scheduler warm up
        loss: "CE"           # can change to unbiased estimators here?
        L2_reg: 0.1          # weight decay
        patience: 2           # early stopping
        num_workers: 0        # number of dataloader workers
        momentum: 0.9     # not used for adam

# query strategy config
query_strategy:
  query_type: "entropy"      # entropy, random, lc, margin, bald, batchbald, coreset
  query_size: 50             # number of points to label per acquisition step
  num_queries: 8            # if 0 then continue until all pool is labelled
  num_subsample: 5000        # score only a select subsample
  num_mc_iters: 20          # number of monte-carlo samples for dropout

# ray config
