program: cluster_experiment_runner.py
entity: ucl-msc-al-benchmarks
project: al-benchmarks
name: rnn-hyper-tune

method: random    # bayes
metric:
  goal: minimize
  name: acq_model/val_loss

parameters:
  # optim
  batch_size:
    values: [4, 8, 16, 32, 64]
  lr:
    # log uniform requires min and max to be exponential powers i.e. searches between exp(min) and exp(max)
    distribution: log_uniform     
    # min: 0.00001
    # max: 0.1
    min: -11.5
    max: -2.3
    # values: [0.001, 0.01, 0.003]
  L2_reg:
    distribution: log_uniform
    # min: 0.000001
    # max: 0.5
    min: -13.8
    max: -0.693
  
  # architecture
  dropout:
    distribution: uniform
    min: 0.0
    max: 0.6
    # values: [0.15, 0.2, 0.25, 0.3, 0.4]
  hid_dim:
    # distribution: q_log_uniform
    # min: 8
    # max: 1024
    values: [8, 16, 32, 64, 128, 256, 512]
  head_dim:
    # distribution: q_log_uniform
    # min: 8
    # max: 1024
    values: [8, 16, 32, 64, 128, 256, 512]
  fine_tune:
    values: ["true", "false"]
  
# for BERT
  # scheduler:
  #   values: ["constant", "linear_warm_up"]
  # warmup_steps:
  #   min: 0
  #   max: 500
  # optimizer:
  #   values: ["adam", "adamw", "SGD"]

# early_terminate:
#   type: hyperband
#   s: 2
#   eta: 3
#   max_iter: 27

# do not include environment as already given by shell script
command:
  - python3
  - "-u"
  - ${program}
  - "--config"
  - "experiments/hyper-tune-config.yaml"
  - ${args}   # this is all the parameters in the sweep config